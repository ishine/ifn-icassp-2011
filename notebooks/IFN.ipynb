{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import mixture, discriminant_analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Smean(f0_cntr):\n",
    "    return np.mean(f0_cntr)\n",
    "\n",
    "def get_Sdmean(f0_cntr):\n",
    "    return np.mean(np.gradient(f0_cntr))\n",
    "\n",
    "def get_Sstd(f0_cntr):\n",
    "    return np.std(f0_cntr)\n",
    "\n",
    "def get_Sdstd(f0_cntr):\n",
    "    return np.std(np.gradient(f0_cntr))\n",
    "\n",
    "def get_Srange(f0_cntr):\n",
    "    return np.max(f0_cntr) - np.max(f0_cntr)\n",
    "\n",
    "def get_Sdrange(f0_cntr):\n",
    "    grad = np.gradient(f0_cntr)\n",
    "    return np.max(grad) - np.min(grad)\n",
    "\n",
    "def get_Smax(f0_cntr):\n",
    "    return np.max(f0_cntr)\n",
    "\n",
    "def get_Smin(f0_cntr):\n",
    "    return np.min(f0_cntr)\n",
    "\n",
    "\n",
    "def get_SQ25(f0_cntr):\n",
    "    return np.quantile(f0_cntr, 0.25)\n",
    "\n",
    "def get_SQ75(f0_cntr):\n",
    "    return np.quantile(f0_cntr, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = {\n",
    "    \"Smean\" : get_Smean,\n",
    "    \"Sdmean\" : get_Sdmean,\n",
    "    \"Sstd\" : get_Sstd,\n",
    "    \"Sdstd\" : get_Sdstd,\n",
    "    \"Srange\" : get_Srange,\n",
    "    \"Sdrange\" : get_Sdrange,\n",
    "    \"Smax\" : get_Smax,\n",
    "    \"Smin\" : get_Smin,\n",
    "    \"SQ25\" : get_SQ25,\n",
    "    \"SQ75\" : get_SQ75\n",
    "}\n",
    "\n",
    "def feat_ext(corpus, features, save=None):\n",
    "    samples = list()\n",
    "    for i in range(len(corpus)):\n",
    "        sample = list()\n",
    "        for j in features:\n",
    "            sample.append(features[j](corpus[i]))\n",
    "        samples.append(sample)\n",
    "\n",
    "    samples = np.array(samples)\n",
    "    if save is not None:\n",
    "        with open(save, \"wb\") as f:\n",
    "            pickle.dump(samples, f)\n",
    "\n",
    "    return samples\n",
    "\n",
    "class IFN(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, ref_corpus, enable_neutral, features, max_iter=10, \\\n",
    "                 ss_iter=400, neutral_label=\"01\", emo_label=\"00\", analysis=True, \\\n",
    "                 analysis_test_size=0.3, norm_scheme=\"ifn\", neu_threshold=0.7, \\\n",
    "                 spkr_file_threshold=0.2, switch_threshold=0.05, log_dir=\"./\", \\\n",
    "                 ref_gmms=None, ldc=None):\n",
    "        \n",
    "        self.enable_neutral = enable_neutral\n",
    "        self.ref_corpus = ref_corpus # should be a list of ref f0 contours\n",
    "        self.ref_feat = None\n",
    "        self.features = features\n",
    "        self.neutral_label = neutral_label\n",
    "        self.emo_label = emo_label\n",
    "        \n",
    "        self.X_train_raw = None\n",
    "        self.y_train_raw = None\n",
    "        \n",
    "        # This flag will trigger treatment of training corpus as both \n",
    "        # training and validation. Fit method will calculate stats of \n",
    "        # validation set after every ss_iter and save the best model.\n",
    "        # The results of every ss_iter will also be saved.\n",
    "        self.analysis = analysis \n",
    "        \n",
    "        # Logs to store all necessary results\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.analysis_test_size = analysis_test_size\n",
    "        \n",
    "        self.ref_gmms = list()\n",
    "        if ref_gmms is not None:\n",
    "            self.ref_gmms = ref_gmms\n",
    "            \n",
    "        \n",
    "        \n",
    "        if self.enable_neutral:\n",
    "            \n",
    "            def fit_ref(self):\n",
    "                    \n",
    "                X_ref = feat_ext(samples, self.features)\n",
    "                \n",
    "                self.ref_feat = X_ref.copy()\n",
    "                \n",
    "                y_ref = np.array([self.neutral_label] * len(samples))\n",
    "                \n",
    "                for i in range(len(self.features)):\n",
    "                    gmm = GaussianMixture(n_components=2)\n",
    "                    gmm.fit(X_ref[:, i].reshape(X_ref.shape[0], 1), y_ref)\n",
    "                    \n",
    "                    self.ref_gmms.append(gmm)\n",
    "                \n",
    "        self.ss_iter = ss_iter\n",
    "        self.max_iter = max_iter\n",
    "        self.balance = False\n",
    "        \n",
    "        self.norm_constants = dict()\n",
    "        self.norm_scheme = norm_scheme\n",
    "        self.neu_threshold = neu_threshold\n",
    "        \n",
    "        self.spkr_file_threshold = spkr_file_threshold\n",
    "        self.switch_threshold = switch_threshold\n",
    "        \n",
    "        self.best_ldc = ldc\n",
    "        self.ldc = ldc\n",
    "        \n",
    "    def fit(self, X_train_raw, y_train_raw, spkrs, save=None):\n",
    "        \n",
    "        # X_train_raw should be a list of pitch contours\n",
    "        # y_train_raw should be np array of labels\n",
    "        # spkrs should be an np array of speakers\n",
    "        self.X_train_raw = X_train_raw\n",
    "        self.y_train_raw = y_train_raw\n",
    "        self.spkrs = spkrs\n",
    "        \n",
    "\n",
    "        num_neu_samples = len(y_train[y_train == self.neutral_label])\n",
    "        num_emo_samples = len(y_train[y_train != self.neutral_label])\n",
    "        \n",
    "        diff = np.abs(num_emo_samples - num_neu_samples) / num_neu_samples\n",
    "        \n",
    "        if diff > 0.1:\n",
    "            self.balance = True\n",
    "        else:\n",
    "            self.ss_iter = 1\n",
    "        \n",
    "        for i in range(self.ss_iter):\n",
    "            \n",
    "            X_train_raw_ss = self.X_train_raw\n",
    "            y_train_raw_ss = self.y_train_raw\n",
    "            spkrs_ss = self.spkrs\n",
    "            \n",
    "\n",
    "            \n",
    "            if self.balance:\n",
    "            \n",
    "                # get indices of emotional samples in training set\n",
    "                emo_idx = [i for i in range(len(y_train_raw_ss)) if y_train_raw_ss != self.neutral_label]\n",
    "\n",
    "                # get indices of neutral samples in training set\n",
    "                neu_idx = [i for i in range(len(y_train_raw_ss)) if y_train_raw_ss == self.neutral_label]\n",
    "                \n",
    "                neutral_samples_per_speaker = dict()\n",
    "                \n",
    "                for i in neu_idx:\n",
    "                    if spkrs_ss[i] not in neutral_samples_per_speaker:\n",
    "                        neutral_samples_per_speaker[spkrs_ss[i]] = 0\n",
    "                        \n",
    "                    neutral_samples_per_speaker += [spkrs_ss[i]]\n",
    "\n",
    "                # sample population of emotional samples equal in size to neutral samples for each speaker\n",
    "                emo_idx_sample = sum([np.random.choice([emo_idx[j] for i in np.unique(spkrs_ss) \\\n",
    "                                                    for j in emo_idx if spkrs_ss[j] == i], \\\n",
    "                                                       neutral_samples_per_speaker[i])], []).tolist()\n",
    "                \n",
    "                # make undersampled dataset\n",
    "                X_train_raw_ss = [X_train_raw_ss[j] for j in emo_idx_sample + neu_idx]\n",
    "                y_train_raw_ss = np.array([self.emo_label] * len(emo_idx_sample)\\\n",
    "                                       + [self.neutral_label] * len(neu_idx))\n",
    "                spkrs_ss = np.array([spkrs_ss[i] for i in emo_idx_sample + neu_idx])\n",
    "            \n",
    "\n",
    "            \n",
    "            if self.analysis:\n",
    "                X_train_raw, X_test_raw, y_train_raw, y_test_raw, spkrs_train, spkrs_test = train_test_split(X_train_raw_ss,\\\n",
    "                                                                                                             y_train_raw_ss,\\\n",
    "                                                                                                             spkrs_ss,\\\n",
    "                                                                                                             test_size=0.3,\\\n",
    "                                                                                                             stratify=np.vstack((y_train_raw_ss, spkrs_ss)).T)\n",
    "    \n",
    "            \n",
    "            self.norm_constants = {i: 1 for i in np.unique(spkrs_ss)}\n",
    "            \n",
    "            X_train_raw_cur = X_train_raw\n",
    "            X_test_raw_cur = X_test_raw\n",
    "            \n",
    "            y_train_raw_cur = y_train_raw\n",
    "            y_test_raw_cur = y_test_raw\n",
    "            \n",
    "            y_train_raw_prev = None\n",
    "            y_test_raw_prev = None\n",
    "            \n",
    "            change = 0\n",
    "            \n",
    "            best_test_acc = -1\n",
    "            self.ldc = None\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                \n",
    "                # Actual IFN iterations start now\n",
    "                X_train_feat = feat_ext(X_train_raw_cur, self.features)\n",
    "                X_test_feat = feat_ext(X_test_raw_cur, self.features)\n",
    "                \n",
    "                ldc = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "\n",
    "                ldc.fit(X_train_feat, y_train_raw)\n",
    "                \n",
    "                probs_ldc = ldc.predict_proba(X_train_feat)\n",
    "                \n",
    "                spkrs_neu_pred = dict()\n",
    "                \n",
    "                for i in range(len(probs_ldc)):\n",
    "                    if probs_ldc[i] > self.neu_threshold:\n",
    "                        if spkrs_use[i] not in spkrs_neu_pred:\n",
    "                            spkrs_neu_pred[i] = 0\n",
    "                        spkrs_neu_pred[i] += 1\n",
    "                \n",
    "                y_train_raw_prev = y_train_raw_cur.copy()\n",
    "                y_test_raw_prev = y_test_raw_cur.copy()\n",
    "                \n",
    "                for spkr in spkrs_neu_pred:\n",
    "                    \n",
    "                    if spkrs_neu_pred[spkr] / len(spkrs_use[spkrs_train == spkr]) < self.spkr_file_threshold:\n",
    "                        \n",
    "                        argsort_spkr_ll = np.argsort(-1 * probs_ldc[spkrs_train == spkr])\n",
    "                        spkr_neu_num = int(np.ceil(0.2 * len(spkrs_use[spkrs_train == spkr])))\n",
    "                        neu_spkr_idx = argsort_spkr_ll[:spkr_neu_num]\n",
    "                        emo_spkr_idx = argsort_spkr_ll[spkr_neu_num:]\n",
    "                        \n",
    "                        \n",
    "                        y_train_raw_cur[neu_spkr_idx] = self.neutral_label\n",
    "                        y_train_raw_cur[emo_spkr_idx] = self.emo_label\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        if self.norm_scheme == \"ifn\":\n",
    "                            \n",
    "                            ref_avg_f0 = sum([sum(self.ref_corpus[i]) / len(self.ref_corpus[i])\\\n",
    "                                          for i in range(len(self.ref_corpus))]) / len(self.ref_corpus)\n",
    "                        \n",
    "                            neu_spkr_f0 = sum([sum(i) for i in X_train_raw[(spkrs_train == spkr) & (probs_ldc > self.neu_threshold)]]\\\n",
    "                                          / len(i)) / len(X_train_raw[(spkrs_train == spkr) & (probs_ldc > self.neu_threshold)])\n",
    "                        \n",
    "                            self.norm_constants[spkr] = ref_avg_f0 / neu_spkr_f0\n",
    "                            \n",
    "                            \n",
    "                            X_train_raw_cur[spkrs_train == spkr] = X_train_raw[spkr_train == spkr] / self.norm_constants[spkr]\n",
    "                        \n",
    "                        elif self.norm_scheme == \"opt\":\n",
    "                            \n",
    "                            ref_avg_f0 = sum([sum(self.ref_corpus[i]) / len(self.ref_corpus[i])\\\n",
    "                                          for i in range(len(self.ref_corpus))]) / len(self.ref_corpus)\n",
    "                        \n",
    "                            neu_spkr_f0 = sum([sum(i) for i in X_train_raw[(spkrs_train == spkr) & (y_train_raw == self.neutral_label)]]\\\n",
    "                                          / len(i)) / len(X_train_raw[(spkrs_train == spkr) & (y_train_raw == self.neutral_label)])\n",
    "                        \n",
    "                            self.norm_constants[spkr] = ref_avg_f0 / neu_spkr_f0\n",
    "                            \n",
    "                            \n",
    "                            X_train_raw_cur[spkrs_train == spkr] = X_train_raw[spkr_train == spkr] / self.norm_constants[spkr]\n",
    "                        \n",
    "                \n",
    "                        elif self.norm_scheme == \"none\":\n",
    "                            \n",
    "                            continue\n",
    "                            \n",
    "                change = np.sum(y_train_raw_cur == y_train_raw_prev) / len(y_train_raw_prev)\n",
    "                \n",
    "                self.ldc = ldc \n",
    "                \n",
    "                clf_report = None\n",
    "                \n",
    "                if self.analysis:\n",
    "                    ldc_prob_test = ldc.predict_proba(X_test_feat)\n",
    "                    ldc_labels_test = np.int64(ldc_prob_test > self.neu_threshold)\n",
    "                    clf_report = classification_report(y_test_raw, ldc_labels_test, output_dict=True)\n",
    "                    \n",
    "                    if clf_report[\"accuracy\"] > best_test_acc:\n",
    "                        best_test_acc = clf_report[\"accuracy\"]\n",
    "                        self.best_ldc = ldc\n",
    "                    \n",
    "                \n",
    "                if change <= self.switch_threshold:\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    ss_iter_dir = self.log_dir + \"ss_iter_\" + str(ss_iter) + \"/\"\n",
    "                    if not os.path.exists(ss_iter_dir):\n",
    "                        os.mkdir(ss_iter_dir)\n",
    "                    \n",
    "                    ifn_iter_dir = ss_iter_dir + str(i) + \"/\"\n",
    "                    if not os.path.exists(ifn_iter_dir):\n",
    "                        os.mkdir(ifn_iter_dir)\n",
    "                    \n",
    "                    with open(ifn_iter_dir + \"clf_report.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(clf_report, f)\n",
    "                    \n",
    "                    with open(ifn_iter_dir + \"ref_gmms.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(self.ref_gmms, f)\n",
    "                        \n",
    "                    with open(ifn_iter_dir + \"ldc.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(self.ldc, f)\n",
    "                        \n",
    "                    with open(ifn_iter_dir + \"best_ldc.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(self.best_ldc, f)\n",
    "                        \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((np.ones((3, 3)), np.zeros((4, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.argsort(-1 * np.array([1, 2, 3]))[:int(np.ceil(0.2 * 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([1, 2, 3]) == np.array([1, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justachetan/.pyenv/versions/3.6.6/envs/afc/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1},\n",
       " '1': {'precision': 0.6666666666666666,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.8,\n",
       "  'support': 2},\n",
       " 'accuracy': 0.6666666666666666,\n",
       " 'macro avg': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.4,\n",
       "  'support': 3},\n",
       " 'weighted avg': {'precision': 0.4444444444444444,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1-score': 0.5333333333333333,\n",
       "  'support': 3}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report([1, 1, 0], [1, 1, 1], output_dict=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2, 3], [4, 2, 1]], [[1]], ['01', '00'], ['00']]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1], [2, 3], [4, 2 ,1]]\n",
    "b = [\"00\", \"01\", \"00\"]\n",
    "train_test_split(a, b, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['aa', 'bb', 'cc', 'dd', 'ee'], dtype='<U2'), array([5, 6, 6, 5, 6])),\n",
       " (array(['aa', 'bb', 'cc', 'dd', 'ee'], dtype='<U2'), array([3, 2, 2, 3, 2])),\n",
       " (array(['aa', 'bb', 'cc', 'dd', 'ee'], dtype='<U2'), array([8, 8, 8, 8, 8])))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [1, 3],\n",
       "       [1, 4],\n",
       "       [0, 4],\n",
       "       [0, 5],\n",
       "       [0, 5],\n",
       "       [0, 6],\n",
       "       [0, 6]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((b, c)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "np.int64(a > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
